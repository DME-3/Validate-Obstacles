{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "from pyproj import Transformer, CRS\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import rasterio\n",
    "import sys\n",
    "import pygmt\n",
    "import utm\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from joblib import Parallel, delayed\n",
    "import joblib.externals.loky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_files_list = \"./download_lists/zone_2.1_files.txt\"\n",
    "zone_laz_dir = \"/media/dimitri/SSD2/Split_NRW/zone_2.1/\"\n",
    "zone_DEM_dir = \"/media/dimitri/SSD2/Split_NRW/zone_2.1_DEM/\"\n",
    "\n",
    "log_dir = \"./logs/\"\n",
    "index_file = \"./assets/index.json\"\n",
    "results_dir = \"./results/\"\n",
    "\n",
    "backup_DEM_file = './DEM_data/urn_eop_DLR_CDEM10_Copernicus_DSM_04_N50_00_E006_00_V8239-2020_1__DEM1__coverage_20231204210410.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for laz processing\n",
    "\n",
    "SSFACTOR = 2 # Subsampling factor for points cloud\n",
    "\n",
    "lastReturnNichtBoden = 20\n",
    "brueckenpunkte = 17\n",
    "unclassified = 1\n",
    "\n",
    "class_ok = [brueckenpunkte, lastReturnNichtBoden, unclassified]\n",
    "\n",
    "dst_crs = 'EPSG:4326'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise logging\n",
    "# Set at DEBUG if necessary or else INFO\n",
    "\n",
    "logging.basicConfig(filename=f'{log_dir}/data_processing_zone_2.1.log', \n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_to_list(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        # Strip newline characters from each line\n",
    "        lines = [line.strip() for line in lines]\n",
    "    return lines\n",
    "\n",
    "# Create a dictionary for quick lookup from the JSON data (much quicker than recursively looking up in the JSON)\n",
    "\n",
    "def create_lookup_dict(json_data):\n",
    "\n",
    "    lookup_dict = {}\n",
    "    for dataset in json_data.get('datasets', []):\n",
    "        for file in dataset.get('files', []):\n",
    "            lookup_dict[file['name']] = (file['size'], file['timestamp'])\n",
    "    return lookup_dict\n",
    "\n",
    "def calculate_size(filenames, lookup_dict):\n",
    "    total_files = 0\n",
    "    total_size = 0\n",
    "    not_found_files = []\n",
    "\n",
    "    for filename in filenames:\n",
    "        file_info = lookup_dict.get(filename)\n",
    "        if file_info:\n",
    "            total_files += 1\n",
    "            total_size += int(file_info[0])  # file_info[0] is the size\n",
    "        else:\n",
    "            not_found_files.append(filename)\n",
    "\n",
    "    return total_files, round(total_size / (1024**3), 2), not_found_files  # Size in GB and list of not found files\n",
    "\n",
    "def check_files_exist(file_list, directory):\n",
    "    missing_files = []\n",
    "    for file in file_list:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        if not os.path.exists(file_path):\n",
    "            missing_files.append(file)\n",
    "    return missing_files\n",
    "\n",
    "def utm_to_latlon(x, y):\n",
    "    # Convert lat/lon to UTM coordinates\n",
    "    lat, lon = utm.to_latlon(x, y, 32, 'U')\n",
    "\n",
    "    return lat, lon\n",
    "\n",
    "def latlon_to_utm(lat, lon):\n",
    "    # Convert lat/lon to UTM coordinates\n",
    "    utm_x, utm_y, _, _ = utm.from_latlon(lat, lon)\n",
    "\n",
    "    return utm_x, utm_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading necessary data and perform verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the index file and create a lookup dictionary\n",
    "\n",
    "with open(index_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "lookup_dict = create_lookup_dict(data)\n",
    "logging.info(\"Index file loaded.\")\n",
    "\n",
    "# Load .laz files list and calculate number of files and size\n",
    "\n",
    "laz_list = load_file_to_list(zone_files_list)\n",
    "index_info = calculate_size(laz_list, lookup_dict)\n",
    "logging.info(f\"Loaded .laz file list {zone_files_list}, found {index_info[0]} files, size is {index_info[1]} GB.\")\n",
    "\n",
    "# Check that all .laz files in the list exist in the index and .laz directory\n",
    "\n",
    "if index_info[2]:\n",
    "    logging.error(f\"The following files were not found in the index: {index_info[2]}\")\n",
    "\n",
    "missing_laz = check_files_exist(laz_list, zone_laz_dir)\n",
    "\n",
    "if not missing_laz:\n",
    "    logging.info(\"All .laz files are present in the LAZ directory.\")\n",
    "else:\n",
    "    logging.error(\"The following .laz files were not found:\", missing_laz)\n",
    "\n",
    "# Create a DEM file list and perform verifications\n",
    "\n",
    "def convert_filenames(laz_files):\n",
    "    dem_files = []\n",
    "    for file in laz_files:\n",
    "        # Split the file name to extract the necessary parts\n",
    "        parts = file.split('_')\n",
    "        # Construct the new file name with the desired format\n",
    "        new_file = f\"dgm1_32_{parts[2]}_{parts[3]}_1_nw.tif\"\n",
    "        dem_files.append(new_file)\n",
    "    return dem_files\n",
    "\n",
    "dem_list = convert_filenames(laz_list)\n",
    "\n",
    "missing_DEM = check_files_exist(dem_list, zone_DEM_dir)\n",
    "\n",
    "if not missing_DEM:\n",
    "    logging.info(\"All DEM .tif files are present in the DEM directory.\")\n",
    "else:\n",
    "    logging.error(\"The following DEM .tif files were not found:\", missing_DEM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_laz(laz_file):\n",
    "\n",
    "    process_id = os.getpid()\n",
    "\n",
    "    #print(laz_file)\n",
    "\n",
    "    highest_points = pd.DataFrame()\n",
    "\n",
    "    laz_file_path = zone_laz_dir + laz_file\n",
    "\n",
    "    with laspy.open(laz_file_path) as file:\n",
    "        las = file.read()\n",
    "    \n",
    "    logging.debug(f\"File {laz_file} loaded\")\n",
    "\n",
    "    class_val = las.classification[::SSFACTOR]\n",
    "\n",
    "    mask = (np.isin(class_val, class_ok))\n",
    "\n",
    "    points = np.vstack((las.x[::SSFACTOR][mask], las.y[::SSFACTOR][mask], las.z[::SSFACTOR][mask])).transpose()\n",
    "\n",
    "    if len(points) == 0:\n",
    "        logging.info(f\"Found no points to process and no obstacles.\")\n",
    "        print('oops')\n",
    "        return highest_points\n",
    "\n",
    "    DEM_file = zone_DEM_dir + convert_filenames([laz_file])[0]\n",
    "\n",
    "    with rasterio.open(DEM_file) as src:\n",
    "        transform, width, height = calculate_default_transform(\n",
    "            src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "        kwargs = src.meta.copy()\n",
    "        kwargs.update({\n",
    "            'crs': dst_crs,\n",
    "            'transform': transform,\n",
    "            'width': width,\n",
    "            'height': height\n",
    "        })\n",
    "\n",
    "        temp_DEM_file = f'./temp/temp_DEM_file{process_id}.tif'\n",
    "\n",
    "        print(temp_DEM_file)\n",
    "\n",
    "        with rasterio.open(temp_DEM_file, 'w', **kwargs) as dst:\n",
    "            for i in range(1, src.count + 1):\n",
    "                reproject(\n",
    "                    source=rasterio.band(src, i),\n",
    "                    destination=rasterio.band(dst, i),\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=dst_crs,\n",
    "                    resampling=Resampling.nearest)\n",
    "    \n",
    "    logging.debug(\"Saved temporary reprojected DEM file\")\n",
    "\n",
    "\n",
    "    transformer = Transformer.from_pipeline(\n",
    "    f\"+proj=pipeline \"\n",
    "    f\"+step +inv +proj=utm +zone=32 +ellps=WGS84 \"  # Convert from UTM Zone 32N to geographic coordinates\n",
    "    f\"+step +proj=vgridshift +grids=@{temp_DEM_file},{backup_DEM_file} +multiplier = -1 \"  # Vertical grid shift to remove the DEM elevation\n",
    "    )\n",
    "\n",
    "    intermediate_time_start = time.time()\n",
    "    transformed_points = np.array([transformer.transform(xi, yi, zi) for xi, yi, zi in zip(points[:,0], points[:,1], points[:,2])])\n",
    "    intermediate_time_stop = time.time()\n",
    "    intermediate_execution_time = intermediate_time_stop - intermediate_time_start\n",
    "\n",
    "    logging.info(f\"Performed vgridshift for {laz_file} in {round(intermediate_execution_time, 1)} seconds\")\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "    data={\n",
    "        \"x\": points[:,0], #np.array(las.x), # We need UTM coordinates\n",
    "        \"y\": points[:,1], #np.array(las.y), # \n",
    "        \"z\": points[:,2],\n",
    "        \"h\": transformed_points[:,2]\n",
    "    }\n",
    "    )\n",
    "\n",
    "    size_df = sys.getsizeof(df)\n",
    "    logging.debug(f\"Size of the DataFrame: {np.ceil(size_df / (1024*1024))} MB\")\n",
    "\n",
    "    inf_rows = df.isin([np.inf, -np.inf]).any(axis=1)\n",
    "    inf_rows_df = df[inf_rows]\n",
    "    df = df[~inf_rows]\n",
    "    logging.debug(f'Removed {len(inf_rows_df)} rows to the dataframe.')\n",
    "\n",
    "    region = pygmt.info(data=df[[\"x\", \"y\"]], spacing=1)  # West, East, South, North\n",
    "\n",
    "    x_min, x_max, y_min, y_max = list(region)\n",
    "\n",
    "    # Filtering the DataFrame\n",
    "    condition = (abs(df['x'] - x_min) < 1.5) | \\\n",
    "                (abs(df['x'] - x_max) < 1.5) | \\\n",
    "                (abs(df['y'] - y_min) < 1.5) | \\\n",
    "                (abs(df['y'] - y_max) < 1.5)\n",
    "\n",
    "    df_filtered = df[~condition]\n",
    "\n",
    "    # Number of rows removed\n",
    "    logging.debug(f'Removed another {len(df) - len(df_filtered)} points on the edge.')\n",
    "\n",
    "    df_trimmed = pygmt.blockmedian(\n",
    "        data=df_filtered[[\"x\", \"y\", \"h\"]],\n",
    "        T=0.9999,  # 99.99th quantile, i.e. the highest point\n",
    "        spacing=\"1+e\", # 1+e for 1 m # 0.1 increases the size of df but more accurate?\n",
    "        region=region,\n",
    "    )\n",
    "\n",
    "    size_df_trimmed = sys.getsizeof(df_trimmed)\n",
    "    logging.debug(f\"Size of the trimmed dataframe: {np.ceil(size_df_trimmed / (1024*1024))} MB\")\n",
    "\n",
    "    high_points = df_trimmed[df_trimmed['h'] > 60] # Default = 60\n",
    "\n",
    "    if high_points.empty:\n",
    "        logging.info(f\"Found no obstacles.\")\n",
    "        os.remove(temp_DEM_file)\n",
    "        return highest_points\n",
    "    else:\n",
    "        # Assuming that points within 100m of each other belong to the same obstacle\n",
    "        clustering = DBSCAN(eps=45, min_samples=2).fit(high_points[['x', 'y', 'h']]) # TODO: no error if no cluster found # Default = 50\n",
    "\n",
    "        # Add the cluster labels to the high_points DataFrame\n",
    "        high_points = high_points.copy()\n",
    "        high_points['cluster'] = clustering.labels_\n",
    "\n",
    "        # Filter out noise points (DBSCAN labels noise as -1)\n",
    "        obstacles = high_points[high_points['cluster'] != -1]\n",
    "\n",
    "        if obstacles.empty:\n",
    "            logging.info(f\"Found no obstacles.\")\n",
    "            os.remove(temp_DEM_file)\n",
    "            return highest_points\n",
    "        else:\n",
    "            # Find the highest point in each obstacle cluster\n",
    "            highest_points = obstacles.loc[obstacles.groupby('cluster')['h'].idxmax()]\n",
    "\n",
    "            # The resulting DataFrame 'highest_points' contains the coordinates of the highest point of each obstacle\n",
    "            highest_points.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # Apply the conversion function to the DataFrame to create new columns 'lat' and 'lon'\n",
    "            highest_points['lat'], highest_points['lon'] = zip(*highest_points.apply(lambda row: utm_to_latlon(row['x'], row['y']), axis=1))\n",
    "\n",
    "            highest_points['gnd_elev'] = highest_points.apply(lambda row: round(-1 * transformer.transform(row['x'], row['y'], 0)[2], 2), axis=1)\n",
    "            highest_points['source'] = laz_file\n",
    "            highest_points['timestamp'] = lookup_dict.get(laz_file)[1]\n",
    "\n",
    "            logging.info(f\"Found {len(highest_points)} obstacles.\")\n",
    "\n",
    "            os.remove(temp_DEM_file)\n",
    "            return highest_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oops\n",
      "oops\n",
      "oops\n",
      "oops\n",
      "oops\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oops\n",
      "oops\n",
      "oops\n",
      "oops\n",
      "oops\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m----> 8\u001b[0m individual_dfs \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m99999\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_laz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlaz_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlaz_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlaz_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(individual_dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m results_df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Dev/Validate-Obstacles/.venv/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dev/Validate-Obstacles/.venv/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Dev/Validate-Obstacles/.venv/lib/python3.11/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "joblib.externals.loky.process_executor._MAX_MEMORY_LEAK_SIZE = int(9e11)\n",
    "\n",
    "logging.info(\"Starting to process...\")\n",
    "start_time = time.time()\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "individual_dfs = Parallel(n_jobs=5, timeout = 99999)(delayed(process_laz)(laz_file) for laz_file in tqdm(laz_list))\n",
    "\n",
    "results_df = pd.concat(individual_dfs, ignore_index=True)\n",
    "\n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Compute execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "logging.info(f\"Processed all files. Execution time: {round(execution_time, 1)} seconds\")\n",
    "\n",
    "results_df.to_excel(f'{results_dir}/results_parallel_zone_2.1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgo\u001b[39;00m\n\u001b[1;32m      3\u001b[0m scattermapbox_objects \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m scattermapbox_objects\u001b[38;5;241m.\u001b[39mappend(go\u001b[38;5;241m.\u001b[39mScattermapbox(\n\u001b[1;32m      6\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarkers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     lon\u001b[38;5;241m=\u001b[39mresults_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     hoverinfo\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Only display the text on hover\u001b[39;00m\n\u001b[1;32m     12\u001b[0m ))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "scattermapbox_objects = []\n",
    "\n",
    "scattermapbox_objects.append(go.Scattermapbox(\n",
    "    mode=\"markers\",\n",
    "    lon=results_df['lon'], \n",
    "    lat=results_df['lat'],\n",
    "    marker={'size': 20, 'color': \"red\", 'opacity': 0.5,},\n",
    "    text='Cluster: '+ results_df['cluster'].apply(lambda x: str(x)) + '<br>' + 'Calculated height (m): ' + results_df['h'].apply(lambda x: str(round(x, 1))),\n",
    "    hoverinfo='text'  # Only display the text on hover\n",
    "))\n",
    "'''\n",
    "scattermapbox_objects.append(go.Scattermapbox(\n",
    "    mode=\"markers\",\n",
    "    lon=AIP_df['geoLong'], \n",
    "    lat=AIP_df['geoLat'],\n",
    "    marker={'size': 10, \n",
    "            'symbol': 'circle',\n",
    "            'color': \"blue\",\n",
    "            'opacity': 0.7,\n",
    "           },\n",
    "    text='Name: ' + AIP_df['txtName']+'<br>'+'Published height (m): ' + AIP_df['valHgt (m)'].apply(lambda x: str(round(x, 1))),\n",
    "    hoverinfo='text'  # Only display the text on hover\n",
    "))\n",
    "'''\n",
    "'''scattermapbox_objects.append(go.Scattermapbox(\n",
    "        name = 'Data limits',\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"black\", width=1),\n",
    "        lat=np.array([LAT_MAX, LAT_MAX, LAT_MIN, LAT_MIN, LAT_MAX]),\n",
    "        lon=np.array([LON_MIN, LON_MAX, LON_MAX, LON_MIN, LON_MIN]),\n",
    "        hoverinfo='name',\n",
    "        hoverlabel_namelength=-1   # https://stackoverflow.com/questions/36207887/plot-ly-hover-box-size-attribute\n",
    "    ))\n",
    "'''\n",
    "# Create a scatter plot of the highest points using Plotly with OpenStreetMap background\n",
    "fig = go.Figure(data=scattermapbox_objects)\n",
    "\n",
    "# Set the layout for the map\n",
    "fig.update_layout(\n",
    "    mapbox={\n",
    "        'style': \"open-street-map\",\n",
    "        'center': {'lon': np.mean(results_df['lon']), 'lat': np.mean(results_df['lat'])},\n",
    "        'zoom': 12\n",
    "    },\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Adjust the margins and set the height\n",
    "fig.update_layout(height=800, margin={\"r\":10,\"t\":10,\"l\":10,\"b\":10})\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
